{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Author contributions\n",
    "Please fill out for each of the following parts who contributed to what:\n",
    "- Conceived ideas: \n",
    "- Performed math exercises: \n",
    "- Performed programming exercises:\n",
    "- Contributed to the overall final assignment: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1\n",
    "## Introduction\n",
    "\n",
    "    Hand-in bug-free (try \"Kernel\" > \"Restart & Run All\") and including all (textual as well as figural) output via Brightspace before the deadline (see Brightspace).\n",
    "    \n",
    "Learning goals:\n",
    "1. Get familiar with jupyter notebooks\n",
    "2. Brush up basics of vectors and matrices\n",
    "3. Get familiar with python\n",
    "4. Get familiar with activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "For exercises 1-5 you are required to write down derivations explicitly in $\\LaTeX$. Thus, you do not need to write any Python code, but you might find calculating the exercises with Python useful for practice. We denote such exercises with \"$\\LaTeX$ here.\"\n",
    "\n",
    "For exercises 6-8, you do have to write Python code. We denote these with \"## Code here ##\". Make sure that your plots are shown *in* the notebook when we open it; which you can achieve by saving the notebook when all plots are open and handing in this version. \n",
    "\n",
    "As mentioned in the course reader, in every assignment we will check whether your notebook code runs through without errors with the Cell->Run All command. You risk loosing many points if this fails. We will not debug your code. Be sure to restart the notebook kernel from time to time (Kernel->Restart) and before submitting to notice it if you use old variable or function names that are still defined in the notebook kernel, but not anymore in the code. \n",
    "\n",
    "A useful reference for linear algebra is [**The Matrix Cookbook**](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=3274). A good overview for partial derivatives is [Khan Academy: Partial Derivatives](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivatives-and-the-gradient/a/introduction-to-partial-derivatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 1: Vector operations (1 point)\n",
    "Let's look at vectors. Work out this assignment by hand and write down your solution in markdown.\n",
    "\n",
    "Let $\\mathbf{x} = (1,2)^T$ and $\\mathbf{y} = (-1,1)^T$\n",
    "\n",
    "1. How much is $10\\mathbf{x}$?\n",
    "1. What is the length (norm) of the vector $\\mathbf{x}$? Briefly show how to calculate the solution. \n",
    "1. How much is $\\mathbf{x}^T\\mathbf{y}$?\n",
    "1. What is the angle between $\\mathbf{x}$ and $\\mathbf{y}$ in degrees? Briefly show how to calculate the solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1\n",
    "1. $\\LaTeX$ here.\n",
    "1. $\\LaTeX$ here.\n",
    "1. $\\LaTeX$ here.\n",
    "1. $\\LaTeX$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Vectors and matrices (1 point)\n",
    "Let's look at vectors and matrices. Work out this assignment by hand and write down your solution in markdown. \n",
    "\n",
    "Let $\\mathbf{x} = (1,2)^T$ and $\\mathbf{A} = \n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{array}\n",
    "\\right)\n",
    "$.\n",
    "\n",
    "1. Can we compute $\\mathbf{x}\\mathbf{A}$? Why can, or why can't we?\n",
    "1. How much is $\\mathbf{A}\\mathbf{x}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2\n",
    "1. $\\LaTeX$ here.\n",
    "1. $\\LaTeX$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Matrices (1 point)\n",
    "Let's look at matrices. Work out this assignment by hand and write down your solution in markdown. \n",
    "\n",
    "Let $\\mathbf{A} = \n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    "1 & 2 \\\\\n",
    "3 & 4\n",
    "\\end{array}\n",
    "\\right)\n",
    "$ and $\\mathbf{B} = \n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    "5 & 6 \\\\\n",
    "7 & 8\n",
    "\\end{array}\n",
    "\\right)\n",
    "$.\n",
    "\n",
    "1. How much is $AB$?\n",
    "1. How much is $BA$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3\n",
    "1. $\\LaTeX$ here.\n",
    "1. $\\LaTeX$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Partial derivatives (1 point)\n",
    "\n",
    "Let's brush up on partial derivatives. \n",
    "\n",
    "Let $\\mathbf{x} = (x_1,\\ldots,x_i,\\ldots,x_n)^T$ (a vector) and $f(\\mathbf{x}) = \\mathbf{x}^T\\mathbf{x}$. Write down the expression for the partial derivative $\\frac{\\partial f}{\\partial x_i}$. Briefly explain how you arrived at the result. For the mathematicions, all $x_i$ are i.i.d.\n",
    "\n",
    "Hint: How would the function $f(\\mathbf{x})$ look like if it was written with the vector scalars $x_i$ instead of the vector $\\mathbf{x}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4\n",
    "$\\LaTeX$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Gradients (1 point)\n",
    "Often, we need to compute the gradient of a particular function. Given a function $f(x_1,\\ldots,x_n)$, the gradient is just a collection of partial derivatives:\n",
    "\\begin{equation*}\n",
    "\\nabla f = \\left(\\frac{\\partial f}{\\partial x_1}, \\ldots,\\frac{\\partial f}{\\partial x_n}\\right) \\,.\n",
    "\\end{equation*}\n",
    "\n",
    "Let $f(x,y) = - (\\cos^2 x + \\cos^2 y)^2$. \n",
    "\n",
    "Derive the gradient $\\nabla f = \\left(\\frac{\\partial f}{\\partial x},\\frac{\\partial f}{\\partial y}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 5\n",
    "$\\LaTeX$ here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Linear activation function (1 point)\n",
    "Define a python function that computes the *linear activation function* (trivial identity) for any given input. Plot it over the input range $x \\in [-10,10]$, and don't forget to add sensible labels to the axes. \n",
    "\n",
    "Hint: use `np.arange()` with a sensible stepsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear activation function: \n",
    "## Code here ##\n",
    "\n",
    "# Plot activation over given range: \n",
    "## Code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Linear threshold activation function (1 point)\n",
    "Define a python function that computes the *linear threshold activation function* (also known as step activation function) for any given input. This activation function has a parameter $\\theta$, which you can, by default, set to $\\theta=0$. Plot it over the input range $x \\in [-10,10]$, and don't forget to add sensible labels to the axes. \n",
    "\n",
    "Hint: use `np.arange()` with a sensible stepsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear threshold activation function: \n",
    "## Code here ##\n",
    "\n",
    "# Plot activation over given range: \n",
    "## Code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: Sigmoid activation function (1 point)\n",
    "Define a python function that computes the *sigmoid activation function* for any given input. Plot it over the input range $x \\in [-10,10]$, and don't forget to add sensible labels to the axes. \n",
    "\n",
    "Hint: use `np.arange()` with a sensible stepsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear sigmoid activation function: \n",
    "## Code here ##\n",
    "\n",
    "# Plot activation over given range: \n",
    "## Code here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9 (1 point)\n",
    "\n",
    "**1.** The input of the activation function in a simple perceptron (or any regular neural network neuron) is calculated as a weighted sum between each input value $x_i$ and each corresponding weight $w_i$, that is: $\\sum_{i=1}^m w_i x_i $.\n",
    "\n",
    "Calculate the input of the activation function for the given input values ```x_inputs``` and weight values ```weights``` **in a for-loop**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Solution 9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inputs = np.array([4.0,2.0,3.0])\n",
    "weights  = np.array([0.7,0.3,0.2])\n",
    "\n",
    "print(\"Shape of inputs: {}.\".format(x_inputs.shape))\n",
    "print(\"Shape of weights: {}.\".format(weights.shape))\n",
    "\n",
    "activation_input = 0.0\n",
    "\n",
    "# Write a for-loop\n",
    "## Code here ##\n",
    "\n",
    "print(\"The input of the activation function is: {}.\".format(activation_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** For-loops tend to be slow. There is a direct mathematical operation that expresses the same as our weighted sum above. This operation is also efficiently implemented as a ```numpy``` function. \n",
    "\n",
    "How is the operation called? Use the corresponding ```numpy``` function **once** to calculate ```activation_input``` in one line without any for-loop. (this also excludes list comprehensions)\n",
    "\n",
    "Hint: $\\sum_{i=1}^m w_i x_i = \\mathbf{w}^\\top \\mathbf{x}$ \n",
    "\n",
    "Note: this one operation is the prefered operation over using for loops, so use it in all upcoming assignments!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inputs = np.array([4.0,2.0,3.0])\n",
    "weights  = np.array([0.7,0.3,0.2])\n",
    "\n",
    "print(\"Shape of inputs: {}.\".format(x_inputs.shape))\n",
    "print(\"Shape of weights: {}.\".format(weights.shape))\n",
    "\n",
    "# Write a one-liner\n",
    "## Code here ##\n",
    "\n",
    "print(\"The input of the activation function is: {}.\".format(activation_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10 (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** When implementing a full neural network we will have multiple $h_n$ hidden units (think $h_n$ individual perceptrons). In a multi-layer perceptron (a simple fully connected neural network), every hidden unit $h_i$ is connected to all of the $m$ input units, leading to $m \\times h_n$ weights in total. Again, first implement this with for-loops only, and with none of numpy's special mathematical functions. In the example below `weights` represents the weights for 4 hidden units. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inputs = np.array([4.0,2.0,3.0])\n",
    "weights  = np.array([[0.7,0.3,0.2], \n",
    "                     [-0.23,0.42,-0.1], \n",
    "                     [-1.5,-2.3,0.4], \n",
    "                     [0.83,-0.12,-0.7]])\n",
    "\n",
    "print(\"Shape of inputs: {}.\".format(x_inputs.shape))\n",
    "print(\"Shape of weights: {}.\".format(weights.shape))\n",
    "\n",
    "activation_inputs = np.zeros([weights.shape[0],])\n",
    "\n",
    "# Write a for-loop\n",
    "## Code here ##\n",
    "\n",
    "print(\"The inputs of the activation functions for the hidden units are: {}.\".format(activation_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** Now implement the same with the operation you found before. You should only use this function once and should not use *any* for loops.  This also excludes list comprehensions. \n",
    "\n",
    "Note: this one operation is the prefered operation over using for loops, so use it in all upcoming assignments!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 10.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inputs = np.array([4.0,2.0,3.0])\n",
    "weights  = np.array([[0.7,0.3,0.2], \n",
    "                     [-0.23,0.42,-0.1], \n",
    "                     [-1.5,-2.3,0.4], \n",
    "                     [0.83,-0.12,-0.7]])\n",
    "\n",
    "print(\"Shape of inputs: {}.\".format(x_inputs.shape))\n",
    "print(\"Shape of weights: {}.\".format(weights.shape))\n",
    "\n",
    "activation_inputs = np.zeros([weights.shape[0],])\n",
    "\n",
    "# Write a one-liner\n",
    "## Code here ##\n",
    "\n",
    "print(\"The inputs of the activation functions for the hidden units are: {}.\".format(activation_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Usually you would process multiple examples at once (*in a batch*), generating a unit activation individually for every example. `x_inputs` now carries two examples. Now - using only the one-line operation you found before one time - again gather the activations. You should not use *any* for-loops. This also excludes list comprehensions. \n",
    "\n",
    "Note: this one operation is the prefered operation over using for loops, so use it in all upcoming assignments!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_inputs = np.array([[4.0,2.0,3.0], \n",
    "                     [3.0,0.5,4.0]])\n",
    "\n",
    "weights  = np.array([[0.7,0.3,0.2], \n",
    "                     [-0.23,0.42,-0.1], \n",
    "                     [-1.5,-2.3,0.4], \n",
    "                     [0.83,-0.12,-0.7]])\n",
    "\n",
    "print(\"Shape of inputs: {}.\".format(x_inputs.shape))\n",
    "print(\"Shape of weights: {}.\".format(weights.shape))\n",
    "\n",
    "activation_inputs = np.zeros([weights.shape[0], 2])\n",
    "\n",
    "# Write a one-liner\n",
    "## Code here ##\n",
    "\n",
    "print(\"The 2 sets of inputs of the activation functions for the hidden units are: {}.\".format(activation_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
